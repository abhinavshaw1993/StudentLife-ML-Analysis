{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some Initiation Code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>sum</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>median</th>\n",
       "      <th>var</th>\n",
       "      <th>skew</th>\n",
       "      <th>linear_m</th>\n",
       "      <th>linear_c</th>\n",
       "      <th>poly_a</th>\n",
       "      <th>poly_b</th>\n",
       "      <th>...</th>\n",
       "      <th>sum</th>\n",
       "      <th colspan=\"2\" halign=\"left\">mean</th>\n",
       "      <th colspan=\"2\" halign=\"left\">min</th>\n",
       "      <th colspan=\"2\" halign=\"left\">max</th>\n",
       "      <th>sum</th>\n",
       "      <th>dow</th>\n",
       "      <th>previous_stress_level</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>activity_inference</th>\n",
       "      <th>activity_inference</th>\n",
       "      <th>activity_inference</th>\n",
       "      <th>activity_inference</th>\n",
       "      <th>activity_inference</th>\n",
       "      <th>activity_inference</th>\n",
       "      <th>activity_inference</th>\n",
       "      <th>activity_inference</th>\n",
       "      <th>activity_inference</th>\n",
       "      <th>activity_inference</th>\n",
       "      <th>...</th>\n",
       "      <th>sleep_rating</th>\n",
       "      <th>hours_slept</th>\n",
       "      <th>sleep_rating</th>\n",
       "      <th>hours_slept</th>\n",
       "      <th>sleep_rating</th>\n",
       "      <th>hours_slept</th>\n",
       "      <th>sleep_rating</th>\n",
       "      <th>sms_instance</th>\n",
       "      <th>Unnamed: 102_level_1</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2013-03-24</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>23</td>\n",
       "      <td>6</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-03-25</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-03-26</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-03-27</th>\n",
       "      <td>785.0</td>\n",
       "      <td>0.116021</td>\n",
       "      <td>0.478653</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.229109</td>\n",
       "      <td>4.499333</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.007809</td>\n",
       "      <td>-1.305164e-08</td>\n",
       "      <td>0.000120</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-03-28</th>\n",
       "      <td>1145.0</td>\n",
       "      <td>0.142466</td>\n",
       "      <td>0.480309</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.230696</td>\n",
       "      <td>3.928749</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>-0.067551</td>\n",
       "      <td>2.158428e-08</td>\n",
       "      <td>-0.000121</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 102 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          sum               mean                std  \\\n",
       "           activity_inference activity_inference activity_inference   \n",
       "time                                                                  \n",
       "2013-03-24                0.0           0.000000           0.000000   \n",
       "2013-03-25                0.0           0.000000           0.000000   \n",
       "2013-03-26                0.0           0.000000           0.000000   \n",
       "2013-03-27              785.0           0.116021           0.478653   \n",
       "2013-03-28             1145.0           0.142466           0.480309   \n",
       "\n",
       "                       median                var               skew  \\\n",
       "           activity_inference activity_inference activity_inference   \n",
       "time                                                                  \n",
       "2013-03-24                0.0           0.000000           0.000000   \n",
       "2013-03-25                0.0           0.000000           0.000000   \n",
       "2013-03-26                0.0           0.000000           0.000000   \n",
       "2013-03-27                0.0           0.229109           4.499333   \n",
       "2013-03-28                0.0           0.230696           3.928749   \n",
       "\n",
       "                     linear_m           linear_c             poly_a  \\\n",
       "           activity_inference activity_inference activity_inference   \n",
       "time                                                                  \n",
       "2013-03-24           0.000000           0.000000       0.000000e+00   \n",
       "2013-03-25           0.000000           0.000000       0.000000e+00   \n",
       "2013-03-26           0.000000           0.000000       0.000000e+00   \n",
       "2013-03-27           0.000032           0.007809      -1.305164e-08   \n",
       "2013-03-28           0.000052          -0.067551       2.158428e-08   \n",
       "\n",
       "                       poly_b          ...                   sum        mean  \\\n",
       "           activity_inference          ...          sleep_rating hours_slept   \n",
       "time                                   ...                                     \n",
       "2013-03-24           0.000000          ...                   8.0         7.0   \n",
       "2013-03-25           0.000000          ...                   0.0         0.0   \n",
       "2013-03-26           0.000000          ...                   0.0         0.0   \n",
       "2013-03-27           0.000120          ...                   6.0         7.0   \n",
       "2013-03-28          -0.000121          ...                   2.0        10.0   \n",
       "\n",
       "                                min                      max               \\\n",
       "           sleep_rating hours_slept sleep_rating hours_slept sleep_rating   \n",
       "time                                                                        \n",
       "2013-03-24          2.0         7.0          2.0         7.0          2.0   \n",
       "2013-03-25          0.0         0.0          0.0         0.0          0.0   \n",
       "2013-03-26          0.0         0.0          0.0         0.0          0.0   \n",
       "2013-03-27          2.0         7.0          2.0         7.0          2.0   \n",
       "2013-03-28          1.0        10.0          1.0        10.0          1.0   \n",
       "\n",
       "                    sum                  dow previous_stress_level  \n",
       "           sms_instance Unnamed: 102_level_1                        \n",
       "time                                                                \n",
       "2013-03-24           23                    6                   2.0  \n",
       "2013-03-25           24                    0                   1.0  \n",
       "2013-03-26           24                    1                   1.0  \n",
       "2013-03-27           25                    2                   1.0  \n",
       "2013-03-28           25                    3                   4.0  \n",
       "\n",
       "[5 rows x 102 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.model_selection import LeavePGroupsOut\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.feature_selection import f_regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "def get_file_list():\n",
    "    path = \"../data/FixedInterval Agrregates\"\n",
    "    owd = os.getcwd()\n",
    "    os.chdir(path)\n",
    "    file_list = [student for student in os.listdir() if \"student\" in student]\n",
    "    files= [ path+\"/\"+student+\"/one_day_aggregate.csv\" for student in file_list]\n",
    "    os.chdir(owd)\n",
    "    \n",
    "    return files\n",
    "\n",
    "def adjust_stress_values(stress_level):\n",
    "    mapping = {\n",
    "        1:2,\n",
    "        2:3,\n",
    "        3:4,\n",
    "        4:1,\n",
    "        5:0\n",
    "    }\n",
    "    \n",
    "    return mapping[stress_level]\n",
    "\n",
    "def generate_barplot(dataframe, title=None, xlabel=None, ylabel=None, file_path=None, show_fig=True):\n",
    "    dataframe.reset_index(inplace=True)\n",
    "    dataframe[\"x_label\"] = dataframe[\"level_0\"] + \"_\"  + dataframe[\"level_1\"]\n",
    "    dataframe.drop(columns=[\"level_0\", \"level_1\"], inplace=True)\n",
    "    dataframe.sort_values(by=0, ascending=False, inplace=True)\n",
    "    \n",
    "    # Plotting code.\n",
    "    fig, ax =  plt.subplots(figsize=(22,22))\n",
    "    plt.barh(np.arange(len(dataframe)), dataframe.iloc[:, 0], log=True)\n",
    "    plt.yticks(np.arange(0, 100+1, 1.0))\n",
    "    plt.title(title, fontsize=20)\n",
    "    plt.xlabel(xlabel, fontsize=20)\n",
    "    plt.ylabel(ylabel, fontsize=20)\n",
    "    \n",
    "    y_ticks = dataframe.iloc[:, 1]\n",
    "    y_ticks.reset_index(drop=True, inplace=True)\n",
    "    ax.set_yticklabels(labels=y_ticks)\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.savefig(file_path)\n",
    "    if show_fig:\n",
    "        plt.show()\n",
    "        \n",
    "    plt.close()\n",
    "    \n",
    "def generate_scatter_plot(x, y, pred_values, title=None, xlabel=None, ylabel=None, file_path=None, show=False):\n",
    "    \n",
    "    color_dict = {0:'blue', 1:'green', 2:'yellow', 3:'red', 4:'black'}\n",
    "    label_dict = {0:'Feeling Great', 1:'Feeling Good', 2:'A Little Stressed', 3:'Stressed Out', 4:'Definitly Stressed'}\n",
    "    trans_dict = {0:1, 1: 0.8, 2:0.5, 3: 0.8, 4:0.9}\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(14,14))\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    \n",
    "    for stress_level in label_dict:\n",
    "        mask = pred_values == stress_level\n",
    "        plt.scatter(x[mask], y[mask], c=color_dict[stress_level], label=label_dict[stress_level])\n",
    "    \n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.savefig(file_path)\n",
    "    if show:\n",
    "        plt.show()\n",
    "    plt.close()\n",
    "\n",
    "max_features = len(train_x_final.columns)\n",
    "# max_features = 2\n",
    "\n",
    "def get_data(stress_agg='min', verbose=False, scaling=False, previous_stress=False):\n",
    "\n",
    "    train_data = pd.DataFrame()\n",
    "    file_list = get_file_list()\n",
    "    file_list = file_list[:1]\n",
    "    student_count = len(file_list)\n",
    "\n",
    "    for file in file_list:\n",
    "        temp_data = pd.read_csv(file,\n",
    "                                index_col=0,\n",
    "                                header=[0, 1])\n",
    "        \n",
    "        \n",
    "        if previous_stress:\n",
    "            # Modelling prevoius stress level to the data frame\n",
    "            previous_stress_levels = temp_data.loc[:, (\"stress_level\", stress_agg)]\n",
    "            previous_stress_levels_len = len(previous_stress_levels)\n",
    "            previous_stress_levels = [2] + list(previous_stress_levels)\n",
    "            previous_stress_levels = previous_stress_levels[:previous_stress_levels_len]\n",
    "            index = len(temp_data.columns)-3\n",
    "            \n",
    "            temp_data.insert(index, \"previous_stress_level\", previous_stress_levels)\n",
    "#             temp_data = temp_data[temp_data.previous_stress_level != 100]\n",
    "            \n",
    "        \n",
    "        train_data = train_data.append(temp_data)\n",
    "    \n",
    "    # Fixing Inf Values, NaN values in df.\n",
    "    train_data.replace(np.inf, 10000000, inplace=True)\n",
    "    train_data.replace(-np.inf, -10000000, inplace=True)\n",
    "    train_data.fillna(method='pad', inplace=True)\n",
    "    train_data.fillna(value=0, inplace=True)\n",
    "    \n",
    "    # Slicing our train set into labels and training data.\n",
    "    students, train_x, train_y = train_data.iloc[:, 0], train_data.iloc[:, 1:-3], train_data.loc[:, (\"stress_level\", stress_agg)]\n",
    "    \n",
    "    # Calculating Label Distribution for train and test.\n",
    "    train_label_dist = train_y.value_counts()\n",
    "    \n",
    "    # Adjusting the Stress values to a scale from 0-4.    \n",
    "    train_y = train_y.apply(adjust_stress_values)\n",
    "    \n",
    "    if verbose:\n",
    "        display(train_y.value_counts().sort_index())\n",
    "        display(train_x.head(2))\n",
    "        display(train_y.head(2))\n",
    "    \n",
    "    if scaling:\n",
    "        # Transforming Data by getting custom transformer.\n",
    "        transformer = StandardScaler()\n",
    "        transforemd_train_x = transformer.fit_transform(train_x)\n",
    "#         transforemd_train_x = normalize(train_x)\n",
    "        train_x = pd.DataFrame(transforemd_train_x, columns=train_x.columns)\n",
    "    \n",
    "    return students, train_x, train_y\n",
    "\n",
    "students, train_x, train_y = get_data(scaling=False, verbose=False, previous_stress=True)\n",
    "display(train_x.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing the variances of each Columns. By looking at the variances of few of the feature, we find that most of them are pretty low. We can chose only the ones that have a variance greated that 0.1.\n",
    "# Doing Feature Selection using a Variance Threshold.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x25a81b8ff28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "variance = train_x.var(axis=0)\n",
    "\n",
    "# Plotting HistoGram of Variances.\n",
    "plt.figure(figsize=(8,4))\n",
    "variance.plot.hist(bins=50)\n",
    "plt.title(\"Number of Features with their respective variances. 102 Total Features.\")\n",
    "plt.xlabel(\"Variance\")\n",
    "plt.savefig(\"./FeatureImportance Plots/VarHistPlot.jpg\")\n",
    "plt.show()\n",
    "\n",
    "# Seeing Variance of each column.\n",
    "variance = pd.DataFrame(variance)\n",
    "    \n",
    "generate_barplot(variance,\n",
    "                title=\"Plot Ranking Each Feature Using Variance as a Metric\",\n",
    "                xlabel=\"Variance\",\n",
    "                ylabel=\"Features\",\n",
    "                file_path=\"./FeatureImportance Plots/FeatureSelectionUsingVarMetric.jpg\",\n",
    "                show_fig=False\n",
    "               )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection_var(train_x, threshold=5,verbose=False):\n",
    "    \n",
    "    var_selector = VarianceThreshold(threshold=threshold)\n",
    "    var_selector.fit(train_x)\n",
    "    selected = var_selector.get_support(indices=True)\n",
    "    not_selected = [i for i in range(len(train_x.columns)) if i not in selected ]\n",
    "    result = train_x.iloc[:, selected]\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Selected :{} features form :{}\".format( len(selected), len(train_x.columns)))\n",
    "        print(\"Features Selected are:\")\n",
    "    \n",
    "    selected = list(train_x.iloc[:, selected].columns)\n",
    "    not_selected = list(train_x.iloc[:, not_selected].columns)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Selected Features\")\n",
    "        for feature in selected:\n",
    "            print(feature)\n",
    "\n",
    "        print(\"\\n\")\n",
    "        print(\"Not Selected Features\")\n",
    "        for feature in not_selected:\n",
    "            print(feature)\n",
    "\n",
    "    return result\n",
    "              \n",
    "# refine_train_x = feature_selection_var(train_x,threshold=5, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection using Univariate Statistical Importance Classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Selection Using Mutual Information.\n",
    "Link - https://en.wikipedia.org/wiki/Mutual_information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "def feature_selection_mutualinfo_classif(train_x, train_y, verbose=False):\n",
    "    \n",
    "    mi = pd.DataFrame(mutual_info_classif(train_x, train_y), index=train_x.columns) \n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Mutual Information in:\")\n",
    "        display(mi)\n",
    "\n",
    "    return mi\n",
    "\n",
    "mi_classif = feature_selection_mutualinfo_classif(train_x, train_y, verbose=False)\n",
    "\n",
    "\n",
    "generate_barplot(mi_classif,\n",
    "                title=\"Plot Ranking Each Feature Using Mutual Information Classification (Corelation Based)\",\n",
    "                xlabel=\"Score\",\n",
    "                ylabel=\"Features\",\n",
    "                file_path=\"./FeatureImportance Plots/FeatureSelectionUsingMIClassif.jpg\",\n",
    "                show_fig=False\n",
    "               )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Selection using F-test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhinav Shaw\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [ 3 11] are constant.\n",
      "  UserWarning)\n",
      "C:\\Users\\Abhinav Shaw\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\Users\\Abhinav Shaw\\Anaconda3\\lib\\site-packages\\matplotlib\\scale.py:111: RuntimeWarning: invalid value encountered in less_equal\n",
      "  out[a <= 0] = -1000\n"
     ]
    }
   ],
   "source": [
    "def feature_selection_f_classif(train_x, train_y, verbose=False):\n",
    "    \n",
    "    f_values, p_values = f_classif(train_x, train_y)\n",
    "    f_values= pd.DataFrame(f_values, index=train_x.columns) \n",
    "    p_values= pd.DataFrame(p_values, index=train_x.columns) \n",
    "    \n",
    "    if verbose:\n",
    "        print(\"F-values and P-values are: \")\n",
    "        print_df = pd.concat([f_values, p_values], axis=1)\n",
    "        display(print_df)\n",
    "\n",
    "    return f_values, p_values\n",
    "\n",
    "f_values, p_values = feature_selection_f_classif(train_x, train_y, verbose=False)\n",
    "\n",
    "generate_barplot(f_values,\n",
    "                title=\"Plot Ranking Each Feature F-test Classification\",\n",
    "                xlabel=\"Score\",\n",
    "                ylabel=\"Features\",\n",
    "                file_path=\"./FeatureImportance Plots/FeatureSelectionUsingF_Test_classification.jpg\",\n",
    "                show_fig=False\n",
    "               )\n",
    "\n",
    "generate_barplot(p_values,\n",
    "                title=\"Plot Ranking Each Feature P-values Classification\",\n",
    "                xlabel=\"Score\",\n",
    "                ylabel=\"Features\",\n",
    "                file_path=\"./FeatureImportance Plots/FeatureSelectionUsingP_values_classification.jpg\",\n",
    "                show_fig=False\n",
    "               )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection for Regression.\n",
    "Feature Selection using Mutual Information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhinav Shaw\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "def feature_selection_mutualinfo_regress(train_x, train_y, verbose=False):\n",
    "    \n",
    "    mi = pd.DataFrame(mutual_info_regression(train_x, train_y), index=train_x.columns) \n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Mutual Information in:\")\n",
    "        display(mi)\n",
    "\n",
    "    return mi\n",
    "\n",
    "mi_regress = feature_selection_mutualinfo_regress(train_x, train_y, verbose=False)\n",
    "\n",
    "generate_barplot(mi_regress,\n",
    "                title=\"Plot Ranking Each Feature Using Mutual Information Regression (Corelation Based)\",\n",
    "                xlabel=\"Score\",\n",
    "                ylabel=\"Features\",\n",
    "                file_path=\"./FeatureImportance Plots/FeatureSelectionUsingMIRegression.jpg\",\n",
    "                show_fig=False\n",
    "               )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Selection using F-regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhinav Shaw\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:298: RuntimeWarning: invalid value encountered in true_divide\n",
      "  corr /= X_norms\n",
      "C:\\Users\\Abhinav Shaw\\Anaconda3\\lib\\site-packages\\scipy\\stats\\_distn_infrastructure.py:879: RuntimeWarning: invalid value encountered in greater\n",
      "  return (self.a < x) & (x < self.b)\n",
      "C:\\Users\\Abhinav Shaw\\Anaconda3\\lib\\site-packages\\scipy\\stats\\_distn_infrastructure.py:879: RuntimeWarning: invalid value encountered in less\n",
      "  return (self.a < x) & (x < self.b)\n",
      "C:\\Users\\Abhinav Shaw\\Anaconda3\\lib\\site-packages\\scipy\\stats\\_distn_infrastructure.py:1821: RuntimeWarning: invalid value encountered in less_equal\n",
      "  cond2 = cond0 & (x <= self.a)\n",
      "C:\\Users\\Abhinav Shaw\\Anaconda3\\lib\\site-packages\\matplotlib\\scale.py:111: RuntimeWarning: invalid value encountered in less_equal\n",
      "  out[a <= 0] = -1000\n"
     ]
    }
   ],
   "source": [
    "def feature_selection_f_regress(train_x, train_y, verbose=False):\n",
    "    \n",
    "    f_values, p_values = f_regression(train_x, train_y)\n",
    "    f_values= pd.DataFrame(f_values, index=train_x.columns) \n",
    "    p_values= pd.DataFrame(p_values, index=train_x.columns) \n",
    "    \n",
    "    if verbose:\n",
    "        print(\"F-values and P-values are: \")\n",
    "        print_df = pd.concat([f_values, p_values], axis=1)\n",
    "        display(print_df)\n",
    "\n",
    "    return f_values, p_values\n",
    "\n",
    "f_values, p_values = feature_selection_f_regress(train_x, train_y, verbose=False)\n",
    "\n",
    "generate_barplot(f_values,\n",
    "                title=\"Plot Ranking Each Feature F-test Regression\",\n",
    "                xlabel=\"Score\",\n",
    "                ylabel=\"Features\",\n",
    "                file_path=\"./FeatureImportance Plots/FeatureSelectionUsingF_Test_regression.jpg\",\n",
    "                show_fig=False\n",
    "               )\n",
    "\n",
    "generate_barplot(p_values,\n",
    "                title=\"Plot Ranking Each Feature P-values Regression\",\n",
    "                xlabel=\"Score\",\n",
    "                ylabel=\"Features\",\n",
    "                file_path=\"./FeatureImportance Plots/FeatureSelectionUsingP_values_regression.jpg\",\n",
    "                show_fig=False\n",
    "               )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pearson Correlation Coefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhinav Shaw\\Anaconda3\\lib\\site-packages\\scipy\\stats\\stats.py:3003: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  r = r_num / r_den\n",
      "C:\\Users\\Abhinav Shaw\\Anaconda3\\lib\\site-packages\\scipy\\stats\\stats.py:5240: RuntimeWarning: invalid value encountered in less\n",
      "  x = np.where(x < 1.0, x, 1.0)  # if x > 1 then return 1.0\n"
     ]
    }
   ],
   "source": [
    "def pearsonr_test(train_x, train_y, verbose=False):\n",
    "    \n",
    "    p_values = []\n",
    "    level_0 = []\n",
    "    level_1 = []\n",
    "    pd.options.display.max_rows = 150\n",
    "    \n",
    "    for column in train_x.columns:\n",
    "        \n",
    "        high, low = column\n",
    "        temp = list(pearsonr(train_x[column], train_y)) \n",
    "        level_0.append(high)\n",
    "        level_1.append(low)\n",
    "        p_values.append(temp) \n",
    "\n",
    "    if verbose:\n",
    "        print(\"P-values are: \")\n",
    "        display(p_values, )\n",
    "    \n",
    "    p_values = pd.DataFrame(p_values, index=[level_0, level_1])\n",
    "    \n",
    "    return p_values\n",
    "\n",
    "p_values = pearsonr_test(train_x, train_y, verbose=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recursive Feature Elimation with Logistic Regression.\n",
    "Selecting top 60 features from Mutual information. Completing Logistic Regression for Scatter plots.\n",
    "Generating Scatter Plots for 20 features from recursive feature elimination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhinav Shaw\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:605: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=4.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_Score : 0.4666666666666667 \n",
      " achieved by estimator :LogisticRegression(C=0.05, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=5,\n",
      "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=100,\n",
      "          solver='liblinear', tol=0.1, verbose=0, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhinav Shaw\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:605: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=4.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Feature Selection best_Score : 0.4666666666666667 \n",
      " achieved by estimator :LogisticRegression(C=0.05, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=5,\n",
      "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=100,\n",
      "          solver='liblinear', tol=0.1, verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "# Selecting the top 60 features using Mutual Information Classif\n",
    "n_features = 60\n",
    "mi_classif = feature_selection_mutualinfo_classif(train_x, train_y, verbose=False)\n",
    "mi_classif.reset_index(drop=True, inplace=True)\n",
    "selected = mi_classif.nlargest(n_features, columns=0).index\n",
    "selected = list(selected)\n",
    "train_x_refined = train_x.iloc[:, selected]\n",
    "\n",
    "# Normalizing the data before training with Logistic Regression.\n",
    "scaler = normalize\n",
    "train_x_refined_norm = pd.DataFrame(scaler(train_x_refined), columns=train_x_refined.columns)\n",
    "\n",
    "# Using Loso Cross Validation with Recursive Feature Elimination.\n",
    "loso = LeaveOneGroupOut()\n",
    "cv = loso.get_n_splits(train_x_refined_norm, train_y, train_y)\n",
    "\n",
    "LogisticRegression_params = {\n",
    "  \"penalty\": ['l1', 'l2'],\n",
    "  \"C\": [5, 2, 1, 0.1,0.05, 0.03, 0.0275,0.025, 0.02 , 0.01, 0.009, 0.0075, 0.005, 0.001],\n",
    "  \"dual\":[False],\n",
    "  \"class_weight\": ['balanced'],\n",
    "  \"random_state\": [100],\n",
    "  \"max_iter\": [5, 10, 15, 20, 30, 50, 80, 100],\n",
    "  \"fit_intercept\": [True, False],\n",
    "  \"tol\": [0.1, 0.05 ,0.01, 0.001, 0.0001, 0.00001]}\n",
    "\n",
    "gridsearch = GridSearchCV(LogisticRegression(), param_grid=LogisticRegression_params, cv=cv, scoring=\"accuracy\")\n",
    "gridsearch.fit(train_x_refined_norm, train_y)\n",
    "best_estimator = gridsearch.best_estimator_\n",
    "\n",
    "print(\"best_Score : {} \\n achieved by estimator :{}\".format(gridsearch.best_score_, gridsearch.best_estimator_))\n",
    "\n",
    "selector = RFE(best_estimator, n_features_to_select=20, step=1)\n",
    "selector.get_params()\n",
    "selector.fit(train_x_refined, train_y)\n",
    "train_x_final = train_x_refined.iloc[:,  selector.support_]\n",
    "train_x_final_norm = train_x_refined_norm.iloc[:,  selector.support_]\n",
    "\n",
    "for i in range(max_features):\n",
    "    for j in range(i, max_features):\n",
    "        if i != j:\n",
    "            \n",
    "            x , y = train_x_final.iloc[:, i], train_x_final.iloc[:, j]\n",
    "            xlabel = \"_\".join(train_x_final.columns[i])\n",
    "            ylabel = \"_\".join(train_x_final.columns[j])\n",
    "            title = \"Plot of Features {} and {}\".format(xlabel, ylabel)\n",
    "            file_path = \"./FeatureImportance Plots/FeatureScatterPlots/LogisticRegressRFE/scatter_classif_{}_{}.jpg\".format(xlabel, ylabel)\n",
    "            generate_scatter_plot(x, y, train_y, title=title, xlabel=xlabel, ylabel=ylabel, file_path=file_path, show=False)\n",
    "\n",
    "\n",
    "# Finally fitting on the selected Features.\n",
    "\n",
    "gridsearch.fit(train_x_final_norm, train_y)\n",
    "best_estimator = gridsearch.best_estimator_\n",
    "print(\"After Feature Selection best_Score : {} \\n achieved by estimator :{}\".format(gridsearch.best_score_, gridsearch.best_estimator_))\n",
    "\n",
    "# best_Score : 0.4888888888888889 \n",
    "#  achieved by estimator :LogisticRegression(C=0.1, class_weight='balanced', dual=False,\n",
    "#           fit_intercept=True, intercept_scaling=1, max_iter=10,\n",
    "#           multi_class='ovr', n_jobs=1, penalty='l2', random_state=100,\n",
    "#           solver='liblinear', tol=0.0001, verbose=0, warm_start=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recursive Feature Elimation with Linear Regression. \n",
    "Selecting top 60 features from Mutual information for Regression.\n",
    "Generating Scatter Plots for 20 features from recursive feature elimination using Linear Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['cv', 'error_score', 'estimator__copy_X', 'estimator__fit_intercept', 'estimator__n_jobs', 'estimator__normalize', 'estimator', 'fit_params', 'iid', 'n_jobs', 'param_grid', 'pre_dispatch', 'refit', 'return_train_score', 'scoring', 'verbose'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhinav Shaw\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "# Selecting the top 60 features using Mutual Information Classif\n",
    "n_features = 60\n",
    "mi_regress = feature_selection_mutualinfo_regress(train_x, train_y, verbose=False)\n",
    "mi_regress.reset_index(drop=True, inplace=True)\n",
    "selected = mi_regress.nlargest(n_features, columns=0).index\n",
    "selected = list(selected)\n",
    "train_x_refined = train_x.iloc[:, selected]\n",
    "\n",
    "# Normalizing the data before training with Logistic Regression.\n",
    "scaler = normalize\n",
    "train_x_refined_norm = pd.DataFrame(scaler(train_x_refined), columns=train_x_refined.columns)\n",
    "\n",
    "\n",
    "# Using Loso Cross Validation with Recursive Feature Elimination.\n",
    "loso = LeaveOneGroupOut()\n",
    "cv = loso.get_n_splits(train_x_refined_norm, train_y, train_y)\n",
    "\n",
    "LinearRegression_params = {\n",
    "  \"penalty\": ['l1', 'l2'],\n",
    "  \"C\": [5, 2, 1, 0.1, 0.01, 0.001],\n",
    "  \"class_weight\": ['balanced'],\n",
    "  \"random_state\": [100]}\n",
    "\n",
    "gridsearch = GridSearchCV(LinearRegression(), param_grid=LogisticRegression_params, cv=cv, scoring=\"accuracy\")\n",
    "\n",
    "print(gridsearch.get_params().keys())\n",
    "# gridsearch.fit(train_x_refined, train_y)\n",
    "# best_estimator = gridsearch.best_estimator_\n",
    "\n",
    "# print(best_estimator)\n",
    "\n",
    "# selector = RFE(best_estimator, n_features_to_select=20, step=1)\n",
    "# selector.get_params()\n",
    "# selector.fit(train_x_refined, train_y)\n",
    "# train_x_final = train_x_refined.iloc[:,  selector.support_]\n",
    "\n",
    "# for i in range(max_features):\n",
    "#     for j in range(i, max_features):\n",
    "#         if i != j:\n",
    "            \n",
    "#             x , y = train_x_final.iloc[:, i], train_x_final.iloc[:, j]\n",
    "#             xlabel = \"_\".join(train_x_final.columns[i])\n",
    "#             ylabel = \"_\".join(train_x_final.columns[j])\n",
    "#             title = \"Plot of Features {} and {}\".format(xlabel, ylabel)\n",
    "#             file_path = \"./FeatureImportance Plots/FeatureScatterPlots/LogisticRegressRFE/scatter_{}_{}.jpg\".format(xlabel, ylabel)\n",
    "#             generate_scatter_plot(x, y, train_y, title=title, xlabel=xlabel, ylabel=ylabel, file_path=file_path, show=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
